\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[legalpaper, margin=1in]{geometry}

\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{physics}

\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{makecell}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{marginnote}
\usepackage{csquotes}
\usepackage{todonotes}

\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{property}{Property}

% Start every section on new page
% \usepackage{titlesec}
% \newcommand{\sectionbreak}{\clearpage}

\title{CSC2516: Programming Assignment 2: Convolutional Neural Networks}
\author{}
\date{February 2021}

\begin{document}

\maketitle

\section*{Part A: Pooling and Upsampling}

\subsection*{1 Complete the model \texttt{PoolUpsampleNet}}

\begin{lstlisting}[language=Python]
#V202102171018
class PoolUpsampleNet(nn.Module):
    def __init__(self, kernel, num_filters, num_colours, num_in_channels):
        super().__init__()

        # Useful parameters
        padding = kernel // 2

        ############### YOUR CODE GOES HERE ############### 
        # Input: [BS, NIC, 32, 32]
        self.block0 = nn.Sequential(
            nn.Conv2d(in_channels=num_in_channels, 
                      out_channels=num_filters, 
                      kernel_size=kernel, 
                      padding=padding),
            nn.MaxPool2d(kernel_size=2),
            nn.BatchNorm2d(num_features=num_filters),
            nn.ReLU()
        )
        # Output: [BS, NF, 16, 16]
        self.block1 = nn.Sequential(
            nn.Conv2d(in_channels=num_filters, 
                      out_channels=2*num_filters, 
                      kernel_size=kernel, 
                      padding=padding),
            nn.MaxPool2d(kernel_size=2),
            nn.BatchNorm2d(num_features=2*num_filters),
            nn.ReLU()
        )
        # Output: [BS, 2NF, 8, 8]
        self.block2 = nn.Sequential(
            nn.Conv2d(in_channels=2*num_filters, 
                      out_channels=num_filters, 
                      kernel_size=kernel, 
                      padding=padding),
            nn.Upsample(scale_factor=2),
            nn.BatchNorm2d(num_features=num_filters),
            nn.ReLU()
        )
        # Output: [BS, NF, 16, 16]
        self.block3 = nn.Sequential(
            nn.Conv2d(in_channels=num_filters, 
                      out_channels=num_colours, 
                      kernel_size=kernel, 
                      padding=padding),
            nn.Upsample(scale_factor=2),
            nn.BatchNorm2d(num_features=num_colours),
            nn.ReLU()
        )
        # Output: [BS, NC, 32, 32]
        self.block4 = nn.Sequential(
            nn.Conv2d(in_channels=num_colours, 
                      out_channels=num_colours, 
                      kernel_size=kernel, 
                      padding=padding)
        )
        # Output: [BS, NC, 32, 32]
        ###################################################

    def forward(self, x):
        ############### YOUR CODE GOES HERE ###############
        for block in (self.block0, self.block1, self.block2, 
                      self.block3, self.block4):
            x = block.forward(x)
        return x
        ###################################################
\end{lstlisting}

\subsection*{2 Run main training loop of \texttt{PoolUpsampleNet}}

\begin{lstlisting}[language=Python]
...
Epoch [25/25], Loss: 1.5852, Time (s): 33
Epoch [25/25], Val Loss: 1.5785, Val Acc: 41.6%, Time(s): 34.18
\end{lstlisting}

\includegraphics{A2.1.png}

\includegraphics{A2.2.png}

\begin{quote}
Do these results look good to you?  Why or why not?
\end{quote}

The images appear blurry because in the network they were up-sampled from 8x8 back to 32x32. The background and saddles also appear poorly coloured.

\subsection*{3 Compute the number of weights, outputs, and connections in the model}

\todo{Check Piazza}





\clearpage
\section*{Part B: Strided and Transposed Convolutions}

\subsection*{1 Complete the model \texttt{ConvTransposeNet}}

\begin{lstlisting}[language=Python]
#V202102171234
class ConvTransposeNet(nn.Module):
    def __init__(self, kernel, num_filters, num_colours, num_in_channels):
        super().__init__()

        # Useful parameters
        stride = 2
        padding = kernel // 2
        output_padding = 1

        ############### YOUR CODE GOES HERE ############### 
        # Input: [BS, NIC, 32, 32]
        self.block0 = nn.Sequential(
            nn.Conv2d(in_channels=num_in_channels, 
                      out_channels=num_filters, 
                      kernel_size=kernel, 
                      padding=padding,
                      stride=stride),
            nn.BatchNorm2d(num_features=num_filters),
            nn.ReLU()
        )
        # Output: [BS, NF, 16, 16]
        self.block1 = nn.Sequential(
            nn.Conv2d(in_channels=num_filters, 
                      out_channels=2*num_filters, 
                      kernel_size=kernel, 
                      padding=padding,
                      stride=stride),
            nn.BatchNorm2d(num_features=2*num_filters),
            nn.ReLU()
        )
        # Output: [BS, 2NF, 8, 8]
        self.block2 = nn.Sequential(
            nn.ConvTranspose2d(in_channels=2*num_filters, 
                      out_channels=num_filters, 
                      kernel_size=kernel, 
                      padding=padding,
                      output_padding=output_padding,
                      stride=stride),
            nn.BatchNorm2d(num_features=num_filters),
            nn.ReLU()
        )
        # Output: [BS, NF, 16, 16]
        self.block3 = nn.Sequential(
            nn.ConvTranspose2d(in_channels=num_filters, 
                      out_channels=num_colours, 
                      kernel_size=kernel, 
                      padding=padding,
                      output_padding=output_padding,
                      stride=stride),
            nn.BatchNorm2d(num_features=num_colours),
            nn.ReLU()
        )
        # Output: [BS, NC, 32, 32]
        self.block4 = nn.Sequential(
            nn.Conv2d(in_channels=num_colours, 
                      out_channels=num_colours, 
                      kernel_size=kernel, 
                      padding=padding)
        )
        # Output: [BS, NC, 32, 32]
        ###################################################

    def forward(self, x):
        ############### YOUR CODE GOES HERE ###############
        for block in (self.block0, self.block1, self.block2, 
                      self.block3, self.block4):
            x = block.forward(x)
        return x
        ###################################################
\end{lstlisting}

\subsection*{Train the model}


\begin{lstlisting}[language=Python]
...
Epoch [25/25], Loss: 1.2070, Time (s): 104
Epoch [25/25], Val Loss: 1.1603, Val Acc: 54.7%, Time(s): 105.78
\end{lstlisting}

\includegraphics{B2.1.png}

\includegraphics{B2.2.png}


\subsection*{3 How do the result compare to Part A?}

The result qualitatively appear similar as before. The current \texttt{ConvTransposeNet} model seems to be worse at colouring brown horses.

The current model resulted in \emph{lower} validation loss (1.2) compared to the previous model (1.6). The \texttt{ConvTranspose2d} layers produce lower loss than \texttt{Upsample} because the transposed convolutions fits additional weights to ``reverse'' the previous skipping done by the convolution, resulting in a better numeric fitting and lower loss. 


\subsection*{4 \texttt{padding} parameter}

\begin{tabular}{ c c c }
	 & \texttt{nn.Conv2d}  & \texttt{nn.ConvTranspose2d} \\ 
	 \hline
	if kernel size = 4 & \texttt{padding}=1 & \makecell{\texttt{padding}=1 \\ \texttt{output\_padding}=0} \\  
	if kernel size = 5 & \texttt{padding}=2 & \makecell{\texttt{padding}=2 \\ \texttt{output\_padding}=1}
\end{tabular}
%\todo{verify}


\subsection*{5 Describe  the  effect  of  batch  sizes  on  the  training/validation loss, and the final image output quality.}

Smaller batch sizes were associated with lower training/validation loss and the output quality was better with smaller batch sizes. 


\clearpage
\section*{Part C: Skip Connections}

\subsection*{1 Add a skip connection...}

\begin{lstlisting}[language=Python]
	#V202102200047
	class UNet(nn.Module):
	    def __init__(self, kernel, num_filters, num_colours, num_in_channels):
	        super().__init__()
	
	        # Useful parameters
	        stride = 2
	        padding = kernel // 2
	        output_padding = 1
	
	        ############### YOUR CODE GOES HERE ############### 
	        # Input: [BS, NIC, 32, 32]
	        self.block0 = nn.Sequential(
	            nn.Conv2d(in_channels=num_in_channels, 
	                      out_channels=num_filters, 
	                      kernel_size=kernel, 
	                      padding=padding),
	            nn.MaxPool2d(kernel_size=2),
	            nn.BatchNorm2d(num_features=num_filters),
	            nn.ReLU()
	        )
	        # Output: [BS, NF, 16, 16]
	        self.block1 = nn.Sequential(
	            nn.Conv2d(in_channels=num_filters, 
	                      out_channels=2*num_filters, 
	                      kernel_size=kernel, 
	                      padding=padding),
	            nn.MaxPool2d(kernel_size=2),
	            nn.BatchNorm2d(num_features=2*num_filters),
	            nn.ReLU()
	        )
	        # Output: [BS, 2NF, 8, 8]
	        self.block2 = nn.Sequential(
	            nn.Conv2d(in_channels=2*num_filters, 
	                      out_channels=num_filters, 
	                      kernel_size=kernel, 
	                      padding=padding),
	            nn.Upsample(scale_factor=2),
	            nn.BatchNorm2d(num_features=num_filters),
	            nn.ReLU()
	        )
	        # Concatenate
	        # Output: [BS, NF+NF, 16, 16]
	        self.block3 = nn.Sequential(
	            nn.Conv2d(in_channels=num_filters+num_filters, 
	                      out_channels=num_colours, 
	                      kernel_size=kernel, 
	                      padding=padding),
	            nn.Upsample(scale_factor=2),
	            nn.BatchNorm2d(num_features=num_colours),
	            nn.ReLU()
	        )
	        # Concatenate
	        # Output: [BS, NC+NIC, 32, 32]
	        self.block4 = nn.Sequential(
	            nn.Conv2d(in_channels=num_colours+num_in_channels, 
	                      out_channels=num_colours, 
	                      kernel_size=kernel, 
	                      padding=padding)
	        )
	        # Output: [BS, NC, 32, 32]
	        ###################################################
	
	    def forward(self, x):
	        ############### YOUR CODE GOES HERE ###############
	        x_input = x
	        x = self.block0(x)
	        x_1 = x
	        x = self.block1(x)
	        x = self.block2(x)
	        x = torch.cat((x_1, x), dim=1)
	        x = self.block3(x)
	        x = torch.cat((x_input, x), dim=1)
	        x = self.block4(x)
	        return x
	        ###################################################
\end{lstlisting}

\subsection*{2 Train the model}


\begin{lstlisting}[language=Python]
	...
	Epoch [25/25], Loss: 1.3075, Time (s): 33
	Epoch [25/25], Val Loss: 1.3323, Val Acc: 48.2%, Time(s): 34.52
\end{lstlisting}

\includegraphics{C2.1.png}

\includegraphics{C2.2.png}

\subsection*{3 How  does  the  result  compare  to  the  previous  mode}

The skip connections did not improve validation loss as compared to the previous model, but improved the output qualitatively. The brown horses were coloured closer to the actual brown colour. The improvements come from:

\begin{enumerate}
\item The skip connections allows high resolution, low level features that did not need convolution operation or only one convolution to be directly passed down to the final layers. 
\item More channels in the final two layers increased the complexity and parameters of the model.
\end{enumerate}




\clearpage
\section*{Part D.1. Fine tune Semantic Segmentation Model with Cross Entropy Loss}



\subsection*{1 Complete the \texttt{train} function}

\begin{lstlisting}[language=Python]
    # We only learn the last layer and freeze all the other weights
    ################ Code goes here ######################
    # Around 2-3 lines of code
    for name, param in model.named_parameters():
        if name.startswith("classifier.4"):
            print("Appending " + name + " to learned_parameters")
            learned_parameters.append(param)
    ######################################################	
\end{lstlisting}



\subsection*{2 Complete the script}


\begin{lstlisting}[language=Python]
# Truncate the last layer and replace it with the new one.
# To avoid `CUDA out of memory` error, you might find it useful (sometimes required)
#   to set the `requires_grad`=False for some layers
################ YOUR CODE GOES HERE ######################
# Around 2 lines of code
for param in model.parameters():
    param.requires_grad = False
model.classifier[-1] = nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
# for i in range(4):
#     model.classifier[i].requires_grad_(False)
# model.backbone.requires_grad_(False)
######################################################
\end{lstlisting}

\begin{lstlisting}[language=Python]
...
Epoch [10/10], Loss: 0.2869, Time (s): 44
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch [10/10], Loss: 0.2661, mIOU: 0.3377, Validation time (s): 12
Saving model...
Best model achieves mIOU: 0.3377


\end{lstlisting}



\includegraphics{D1-2.1.png}

\includegraphics{D1-2.2.png}



\subsection*{3 Visualize Predictions}

\begin{lstlisting}[language=Python]
plot_prediction(args, model, is_train=True, index_list=[0, 1, 2, 3])
\end{lstlisting}

\includegraphics[width=\textwidth]{D1-3.1.1.png}

\includegraphics[width=\textwidth]{D1-3.1.2.png}

\includegraphics[width=\textwidth]{D1-3.1.3.png}

\includegraphics[width=\textwidth]{D1-3.1.4.png}

\begin{lstlisting}[language=Python]
plot_prediction(args, model, is_train=False, index_list=[0, 1, 2, 3])
\end{lstlisting}


\includegraphics[width=\textwidth]{D1-3.2.1.png}

\includegraphics[width=\textwidth]{D1-3.2.2.png}

\includegraphics[width=\textwidth]{D1-3.2.3.png}

\includegraphics[width=\textwidth]{D1-3.2.4.png}




\clearpage
\section*{Part D.2. Finetune Semantic Segmentation Model with IoU Loss}


\subsection*{1 Change the loss function from cross entropy used in part D.1 to the (soft) IoU loss}

\begin{lstlisting}[language=Python]
# def compute_IoU_loss(pred, gt):
def compute_iou_loss(pred, gt):
    # Compute the IoU between the pred and the gt (ground truth)
    ################ YOUR CODE GOES HERE ######################
    # Around 2-3 lines of code

    # - apply softmax on pred along the channel dimension (dim=1)
    softmaxed_pred = nn.functional.softmax(pred, dim=1)

    # - only have to compute IoU between gt and the foreground channel of pred
    # - no need to consider IoU for the background channel of pred
    # - extract foreground from the softmaxed pred (e.g., softmaxed_pred[:, 1, :, :])
    softmaxed_pred_fg = softmaxed_pred[:, 1, :, :]

    # - compute intersection between foreground and gt
    intersection = (softmaxed_pred_fg * gt).sum()

    # - compute union between foreground and gt
    union = (softmaxed_pred_fg + gt - softmaxed_pred_fg * gt).sum()

    # - compute loss using the computed intersection and union
    loss = 1.0 - intersection / union

    ######################################################
    return loss

...

# Truncate the last layer and replace it with the new one.
# To avoid `CUDA out of memory` error, you might find it useful (sometimes required)
#   to set the `requires_grad`=False for some layers
################ YOUR CODE GOES HERE ######################
# Around 2 lines of code
for param in model.parameters():
    param.requires_grad = False
model.classifier[-1] = nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
######################################################
\end{lstlisting}

\begin{quote}
What is the validation mIoU (mean IoU)? How does this compare with the mIoU when training with the cross entropy?
\end{quote}

Best model achieves mIOU: 0.3142. Previously with cross entropy the best model mIOU was 0.3377. The current method achieved a better model than the previous best model. See below:

\begin{lstlisting}[language=Python]
...
Epoch [10/10], Loss: 0.4062, Time (s): 46
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Epoch [10/10], Loss: 0.4022, mIOU: 0.3142, Validation time (s): 12
Saving model...
Best model achieves mIOU: 0.3142
\end{lstlisting}


\includegraphics{D2-1.1.png}

\includegraphics{D2-1.2.png}

\subsection*{2 Visualize the predictions}




\begin{lstlisting}[language=Python]
plot_prediction(args, model, is_train=True, index_list=[0, 1, 2, 3])
\end{lstlisting}

\includegraphics[width=\textwidth]{D2-2.1.1.png}

\includegraphics[width=\textwidth]{D2-2.1.2.png}

\includegraphics[width=\textwidth]{D2-2.1.3.png}

\includegraphics[width=\textwidth]{D2-2.1.4.png}

\begin{lstlisting}[language=Python]
plot_prediction(args, model, is_train=False, index_list=[0, 1, 2, 3])
\end{lstlisting}


\includegraphics[width=\textwidth]{D2-2.2.1.png}

\includegraphics[width=\textwidth]{D2-2.2.2.png}

\includegraphics[width=\textwidth]{D2-2.2.3.png}

\includegraphics[width=\textwidth]{D2-2.2.4.png}


\end{document}
