{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "bRWfRdmVVjUl",
        "0yh08KhgnA30",
        "ecEq4TP2lZ4Z",
        "RWwA6OGqlaTq",
        "AJSafHSAmu_w",
        "73_p8d5EmvOJ",
        "vYPae08Io1Fi",
        "9tcpUFKqo2Oi",
        "z1hDi020rT36",
        "MBnBXRG8mvcn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/csc413-winter-2021/blob/main/a3/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "690b5182-09d4-4697-d478-4ecd4ef5bb8c"
      },
      "source": [
        "#@title\n",
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/80/eca7a2d1a3c2dafb960f32f844d570de988e609f5fd17de92e1cf6a01b0a/Pillow-4.0.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 10.9MB/s \n",
            "\u001b[?25hCollecting olefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow, olefile\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl size=1007391 sha256=b0e5826f1915c03a89624c949deef9690e949f8511eabf0febd9f14b7a797604\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=15593e6cdd75cf9a978d81e82b35b0facf70b1347f33f2fe3ac399260fd00fb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built Pillow olefile\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: olefile, Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0 olefile-0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "#@title\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "#@title\n",
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "#@title\n",
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e576fc0-e6b2-484e-81ae-7e92a36dd941"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        i = torch.sigmoid(self.Wii(x) + \n",
        "                                  self.Whi(h_prev))\n",
        "        f = torch.sigmoid(self.Wif(x) + \n",
        "                                  self.Whf(h_prev))\n",
        "        g = torch.tanh(self.Wig(x) + \n",
        "                    self.Whg(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + \n",
        "                                  self.Who(h_prev))\n",
        "        c_new = f * c_prev + i * g\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "        return h_new, c_new"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cc8ba8-0070-47da-a500-31617f09b32d"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':64,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('messenger', 'essengermay')\n",
            "('they', 'eythay')\n",
            "('exert', 'exertway')\n",
            "('smile', 'ilesmay')\n",
            "('depressed', 'epressedday')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.328 | Val loss: 2.110 | Gen: ay ay-onay ay-onay-onay-ay-ay ay ay-onay-onay\n",
            "Epoch:   1 | Train loss: 1.867 | Val loss: 1.904 | Gen: ay ay-onay ollay-onay-onay-onay ay ongay-onay\n",
            "Epoch:   2 | Train loss: 1.689 | Val loss: 1.768 | Gen: eay alway ollay-ay-onteday ingay onday-ay-ay\n",
            "Epoch:   3 | Train loss: 1.557 | Val loss: 1.663 | Gen: eay ay oodgay-onday-ay-ay ingway oonday-ay\n",
            "Epoch:   4 | Train loss: 1.456 | Val loss: 1.639 | Gen: earway arway ollay-onay-atedway iway oodgay\n",
            "Epoch:   5 | Train loss: 1.379 | Val loss: 1.582 | Gen: eray away ollway-onay-onay iway otay-orway\n",
            "Epoch:   6 | Train loss: 1.300 | Val loss: 1.508 | Gen: eray arway olfsay-ontionway iway ortionway\n",
            "Epoch:   7 | Train loss: 1.217 | Val loss: 1.426 | Gen: eray arway ollway-ationtedway iway ortay-oray\n",
            "Epoch:   8 | Train loss: 1.150 | Val loss: 1.423 | Gen: erway arway onsay-onay-ingway iway orday\n",
            "Epoch:   9 | Train loss: 1.088 | Val loss: 1.395 | Gen: ehay arway onsay-ontionsay iway ordingway\n",
            "Epoch:  10 | Train loss: 1.039 | Val loss: 1.274 | Gen: ehay ariway onsitionsay-atway iway orcingway\n",
            "Epoch:  11 | Train loss: 0.980 | Val loss: 1.270 | Gen: ehway arway ontionsay-ingsay isway ourscay\n",
            "Epoch:  12 | Train loss: 0.942 | Val loss: 1.193 | Gen: ehway irway ontionsay-incay isway orcingway\n",
            "Epoch:  13 | Train loss: 0.899 | Val loss: 1.134 | Gen: ehway ariway ousingitingcay isway oushighay\n",
            "Epoch:  14 | Train loss: 0.842 | Val loss: 1.116 | Gen: ehay ariway ontingsay-ingcay isway orcingway\n",
            "Epoch:  15 | Train loss: 0.800 | Val loss: 1.079 | Gen: ehay ariway ontionstay-inway isway orcousway\n",
            "Epoch:  16 | Train loss: 0.765 | Val loss: 1.056 | Gen: ehtay ariway ousingtay-idsay isway orcighway\n",
            "Epoch:  17 | Train loss: 0.729 | Val loss: 1.050 | Gen: ehtay ariway ondinglyingcay isway oumponway\n",
            "Epoch:  18 | Train loss: 0.713 | Val loss: 1.017 | Gen: ethay iray ondiotionsthay isway orkionway\n",
            "Epoch:  19 | Train loss: 0.674 | Val loss: 1.013 | Gen: ehay ariway ontinglyioncay isway oodgesway\n",
            "Epoch:  20 | Train loss: 0.646 | Val loss: 0.970 | Gen: ethay ariway ondigationstay isway orkionway\n",
            "Epoch:  21 | Train loss: 0.608 | Val loss: 0.920 | Gen: ethay ariway ondiniligateway isway orkignway\n",
            "Epoch:  22 | Train loss: 0.572 | Val loss: 0.933 | Gen: ethay ariway ontinillway-ingay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.553 | Val loss: 0.903 | Gen: ethay ariway ondictinglycay isway orkignway\n",
            "Epoch:  24 | Train loss: 0.539 | Val loss: 0.926 | Gen: ethay ariway ousindway-indingay isway ookersway\n",
            "Epoch:  25 | Train loss: 0.531 | Val loss: 0.910 | Gen: ethay ariway ondintionceway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.521 | Val loss: 0.906 | Gen: ehtay ariway ontinilionthay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.509 | Val loss: 0.903 | Gen: ethay ariway ondilinglytay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.480 | Val loss: 0.848 | Gen: ethay ariway ontinglycionway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.462 | Val loss: 0.841 | Gen: ethay ariway ondigintioncay isway okorseway\n",
            "Epoch:  30 | Train loss: 0.434 | Val loss: 0.800 | Gen: ethay ariway ontinitcingnay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.421 | Val loss: 0.854 | Gen: ethay ariway onditionsay-axlay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.429 | Val loss: 0.824 | Gen: ethay iray ontingicationway isway okorseway\n",
            "Epoch:  33 | Train loss: 0.407 | Val loss: 0.788 | Gen: ethay ariway ontingicationway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.387 | Val loss: 0.831 | Gen: ethay ariway ontinginationway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.375 | Val loss: 0.801 | Gen: ethay ariway ontinicationway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.368 | Val loss: 0.844 | Gen: ehay ariway ondigintioncay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.376 | Val loss: 0.798 | Gen: ethay ariway ontistualingway isway orkingway\n",
            "Epoch:  38 | Train loss: 0.364 | Val loss: 0.872 | Gen: ehay ariway ondintionatifway isway orkingway\n",
            "Epoch:  39 | Train loss: 0.354 | Val loss: 0.780 | Gen: ethay ariway ontingicationway isway orkingway\n",
            "Epoch:  40 | Train loss: 0.327 | Val loss: 0.745 | Gen: ethay ariway ontininoutchay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.313 | Val loss: 0.751 | Gen: ethay ariway ontininitchay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.310 | Val loss: 0.802 | Gen: ethay ariway onditingnatcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.335 | Val loss: 0.907 | Gen: ethay ariway ontistinglycay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.367 | Val loss: 0.848 | Gen: ehay ariway odintingcay-igay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.347 | Val loss: 0.759 | Gen: ethay ariway ontigincuatioway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.312 | Val loss: 0.790 | Gen: ethay ariway onditinglycay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.292 | Val loss: 0.731 | Gen: ethay ariway ontinioustcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.271 | Val loss: 0.739 | Gen: ehay ariway ontiniouslycay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.265 | Val loss: 0.768 | Gen: ethay ariway ontinidgay-ackay isway orkingway\n",
            "Obtained lowest validation loss of: 0.7314756597467774\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway ontinidgay-ackay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3079984a-3290-4448-960b-2deb8b776f5c"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('constrained', 'onstrainedcay')\n",
            "('gout', 'outgay')\n",
            "('rake', 'akeray')\n",
            "('smile', 'ilesmay')\n",
            "('ims', 'imsway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.281 | Val loss: 2.115 | Gen: ay-ay ay-ay-ay elllllay-ontintintay ay-ay ontay-ontay\n",
            "Epoch:   1 | Train loss: 1.825 | Val loss: 1.892 | Gen: onay allay ontintintintay intay ontintay-ay\n",
            "Epoch:   2 | Train loss: 1.650 | Val loss: 1.731 | Gen: eay antay oontinstinway inway ontay-inway\n",
            "Epoch:   3 | Train loss: 1.521 | Val loss: 1.632 | Gen: eay-ay antay ongingingay inway ontay-inway\n",
            "Epoch:   4 | Train loss: 1.413 | Val loss: 1.546 | Gen: eway angay ontinginginway inway ontinway\n",
            "Epoch:   5 | Train loss: 1.300 | Val loss: 1.469 | Gen: ecay ay-inway ontiongistinway inway ontistisway\n",
            "Epoch:   6 | Train loss: 1.200 | Val loss: 1.436 | Gen: ecay arlway ingiongionway isway iongway\n",
            "Epoch:   7 | Train loss: 1.118 | Val loss: 1.388 | Gen: ecay angway ongingingway isway onginway\n",
            "Epoch:   8 | Train loss: 1.049 | Val loss: 1.336 | Gen: etay angway ongingitionway isway onsiblay\n",
            "Epoch:   9 | Train loss: 0.990 | Val loss: 1.300 | Gen: ethay awlay ongingicay-incay isway ongiblay\n",
            "Epoch:  10 | Train loss: 0.943 | Val loss: 1.304 | Gen: ethay inway onginghtinway isway oingeway\n",
            "Epoch:  11 | Train loss: 0.895 | Val loss: 1.242 | Gen: ethay abliway oningicay-incay isway oingeway\n",
            "Epoch:  12 | Train loss: 0.848 | Val loss: 1.226 | Gen: ethay alisway oninghtingway isway orinway-iway\n",
            "Epoch:  13 | Train loss: 0.812 | Val loss: 1.270 | Gen: ethay abliway ongistinglay isway onsilbay\n",
            "Epoch:  14 | Train loss: 0.783 | Val loss: 1.249 | Gen: ethay inway onsingicay-oncay isway oringway\n",
            "Epoch:  15 | Train loss: 0.751 | Val loss: 1.200 | Gen: ethay ialway oningitiontay isway oringway\n",
            "Epoch:  16 | Train loss: 0.712 | Val loss: 1.202 | Gen: ethay ialway oningitiondway isway orinway\n",
            "Epoch:  17 | Train loss: 0.691 | Val loss: 1.122 | Gen: ethay ialway oningitionday isway oringway\n",
            "Epoch:  18 | Train loss: 0.664 | Val loss: 1.160 | Gen: ehtway ainway onsingchancinay isway orsilway\n",
            "Epoch:  19 | Train loss: 0.651 | Val loss: 1.136 | Gen: ethay ialway oninginthenay isway oningeway\n",
            "Epoch:  20 | Train loss: 0.619 | Val loss: 1.112 | Gen: ethay ainway onsinghicationway isway orinway\n",
            "Epoch:  21 | Train loss: 0.589 | Val loss: 1.151 | Gen: ethay ailway ongray-ingecatinway isway orsilway\n",
            "Epoch:  22 | Train loss: 0.580 | Val loss: 1.122 | Gen: ethay ialway onintingitray isway oringway\n",
            "Epoch:  23 | Train loss: 0.566 | Val loss: 1.088 | Gen: ethay ailway onsivingteray isway oringway\n",
            "Epoch:  24 | Train loss: 0.547 | Val loss: 1.083 | Gen: ethay ailway onsivintentay isway ornikway\n",
            "Epoch:  25 | Train loss: 0.536 | Val loss: 1.067 | Gen: ethay airway oniditionecay isway orcinway\n",
            "Epoch:  26 | Train loss: 0.523 | Val loss: 1.098 | Gen: ethay ainway onsivitionatesay isway orinkway\n",
            "Epoch:  27 | Train loss: 0.520 | Val loss: 1.047 | Gen: ethay ialay ondinicationcay isway orcinway\n",
            "Epoch:  28 | Train loss: 0.504 | Val loss: 1.084 | Gen: ethay ailway ondinicay-oncay isway orinway\n",
            "Epoch:  29 | Train loss: 0.479 | Val loss: 1.027 | Gen: ethay ailway onsivingtay-ecay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.464 | Val loss: 1.082 | Gen: ethay ialay onsivintecay-oncay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.450 | Val loss: 1.017 | Gen: ethay ailway onsivintestay isway orcinway\n",
            "Epoch:  32 | Train loss: 0.439 | Val loss: 1.074 | Gen: ethay ialay onsivintecay-oncay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.436 | Val loss: 1.048 | Gen: ethay ailway onsivationcenay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.417 | Val loss: 1.047 | Gen: ethay ialay onsivintecationway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.415 | Val loss: 1.085 | Gen: ehtay airway ondicay-ingeccay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.410 | Val loss: 1.075 | Gen: ethay ialay onsivationsclay isway ornikway\n",
            "Epoch:  37 | Train loss: 0.429 | Val loss: 1.084 | Gen: ehtay ialay onsivingtay-inchay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.415 | Val loss: 1.046 | Gen: ethay ailway onsivintencay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.384 | Val loss: 1.005 | Gen: ethay airway ondicingtray isway orkingway\n",
            "Epoch:  40 | Train loss: 0.368 | Val loss: 1.021 | Gen: ethay ailway onsivationsclay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.358 | Val loss: 1.091 | Gen: ethay ailway onsivintecay-onacay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.361 | Val loss: 1.007 | Gen: ethay ailway onsivitatingray isway orkingway\n",
            "Epoch:  43 | Train loss: 0.351 | Val loss: 1.019 | Gen: ethay airway onsivitationsay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.346 | Val loss: 1.004 | Gen: ethay ailway onsivinatonchay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.342 | Val loss: 1.037 | Gen: ethay airway onsivingationway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.339 | Val loss: 1.037 | Gen: ethay ailway ondicingtercay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.347 | Val loss: 1.058 | Gen: ethay airway onsivinationcay isway orknigway\n",
            "Epoch:  48 | Train loss: 0.343 | Val loss: 1.045 | Gen: ethay ailway ondicingtionay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.328 | Val loss: 0.996 | Gen: ethay airway ondicitioncay isway orkingway\n",
            "Obtained lowest validation loss of: 0.995804995083465\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondicitioncay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "1106e130-188d-42ee-db74-c30b13a7a081"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75156ce-f50d-45e3-e9c4-2dd6659df76b"
      },
      "source": [
        "best_encoder = rnn_encode_s # None # Replace with rnn_losses_s or rnn_losses l\n",
        "best_decoder = rnn_decoder_s # None # etc.\n",
        "best_args = rnn_args_s # None\n",
        "\n",
        "TEST_SENTENCE = 'bond-street print-shops are good-for-nothing' # 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tbond-street print-shops are good-for-nothing \n",
            "translated:\tondertay-inbay inptray-ospatshay areway oodgay-orfay-othingn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
        "            \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eed32fd-c53a-4c2c-9b5b-645b6480e3f3"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stand', 'andstay')\n",
            "('taverns', 'avernstay')\n",
            "('ebullition', 'ebullitionway')\n",
            "('subject', 'ubjectsay')\n",
            "('tranquil', 'anquiltray')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.087 | Val loss: 1.756 | Gen: etay ay-ay ingsay-ontiontiongay isay oray-ontiontay\n",
            "Epoch:   1 | Train loss: 1.511 | Val loss: 1.505 | Gen: etay iay-ay inghay-onday isay ortay\n",
            "Epoch:   2 | Train loss: 1.181 | Val loss: 1.287 | Gen: etway ariway incotingway isway orway-orday\n",
            "Epoch:   3 | Train loss: 0.961 | Val loss: 1.229 | Gen: etway ainway intingnoningway isday oringsay\n",
            "Epoch:   4 | Train loss: 0.835 | Val loss: 1.080 | Gen: othay iday ondintingionday isway orway-onday\n",
            "Epoch:   5 | Train loss: 0.683 | Val loss: 0.909 | Gen: ethay aiway ondingcay-ingway issway oringsay\n",
            "Epoch:   6 | Train loss: 0.545 | Val loss: 0.836 | Gen: etay airway onditiningcay isway orway-ondway\n",
            "Epoch:   7 | Train loss: 0.447 | Val loss: 0.728 | Gen: ethay airway onditiningingway isway oringway\n",
            "Epoch:   8 | Train loss: 0.358 | Val loss: 0.620 | Gen: ethay airway onditiningcay isway oringway\n",
            "Epoch:   9 | Train loss: 0.284 | Val loss: 0.657 | Gen: ethay airway ontiingingcay isway oringway\n",
            "Epoch:  10 | Train loss: 0.237 | Val loss: 0.649 | Gen: ethay airway ondinotinglingway isway orkngway\n",
            "Epoch:  11 | Train loss: 0.227 | Val loss: 0.628 | Gen: ethay airway onditingingingway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.204 | Val loss: 0.634 | Gen: ethay airway ontiningcay-ingway isway orkngway\n",
            "Epoch:  13 | Train loss: 0.209 | Val loss: 0.523 | Gen: ethay airway onditingcingway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.195 | Val loss: 0.443 | Gen: ethay airway onditioningfay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.141 | Val loss: 0.421 | Gen: ethay airway onditingcingway isway orkingway\n",
            "Epoch:  16 | Train loss: 0.101 | Val loss: 0.346 | Gen: ethay airway onditinginglay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.071 | Val loss: 0.383 | Gen: ethay airway onditinglingway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.070 | Val loss: 0.396 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.073 | Val loss: 0.369 | Gen: ethay airway onditingingcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.063 | Val loss: 0.338 | Gen: ethay airway onditingioncay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.054 | Val loss: 0.250 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.031 | Val loss: 0.263 | Gen: ethay airway onditingcingway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.027 | Val loss: 0.257 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.021 | Val loss: 0.234 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.015 | Val loss: 0.208 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.011 | Val loss: 0.207 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.009 | Val loss: 0.204 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.007 | Val loss: 0.203 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.006 | Val loss: 0.204 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.006 | Val loss: 0.203 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.005 | Val loss: 0.202 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.005 | Val loss: 0.201 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.004 | Val loss: 0.200 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.004 | Val loss: 0.200 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.004 | Val loss: 0.199 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.003 | Val loss: 0.198 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.003 | Val loss: 0.198 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.003 | Val loss: 0.197 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.003 | Val loss: 0.197 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.002 | Val loss: 0.196 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.002 | Val loss: 0.195 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.002 | Val loss: 0.195 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.002 | Val loss: 0.195 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.002 | Val loss: 0.194 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.002 | Val loss: 0.194 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.002 | Val loss: 0.193 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.002 | Val loss: 0.193 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.001 | Val loss: 0.192 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.001 | Val loss: 0.192 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.19177007413018146\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Vdt8vioo203E",
        "outputId": "5cb7c41b-70e6-4465-c8a8-de01abb6ae1b"
      },
      "source": [
        "plt.plot(np.array(rnn_attn_losses[0]).reshape(len(rnn_attn_losses[1]), -1).mean(axis=1), label=\"Training\")\n",
        "plt.plot(rnn_attn_losses[1], label=\"Validataion\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"loss_additive_attention.pdf\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5d3d7b-56fb-4831-c190-8d8a55540edc"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, k, hidden_size = queries.shape\n",
        "        batch_size, seq_len, hidden_size = keys.shape\n",
        "        # q = \n",
        "        # k = \n",
        "        # v = \n",
        "        unnormalized_attention = (self.Q(queries) @ \n",
        "                                  self.K(keys).transpose(2,1) * \n",
        "                                  self.scaling_factor)\n",
        "        # assert unnormalized_attention.shape == (batch_size, k, seq_len)\n",
        "        # Softmax over seq_len and dim = 1\n",
        "        attention_weights = self.softmax(unnormalized_attention.transpose(2,1))\n",
        "        # assert attention_weights.shape == (batch_size, seq_len, k)\n",
        "        context = context = attention_weights.transpose(2, 1) @ self.V(values)\n",
        "        # assert context.shape == (batch_size, k, hidden_size)\n",
        "        return context, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "\n",
        "        batch_size, k, hidden_size = queries.shape\n",
        "        batch_size, seq_len, hidden_size = keys.shape\n",
        "        # q = \n",
        "        # k = \n",
        "        # v = \n",
        "        unnormalized_attention = (self.Q(queries) @ \n",
        "                                  self.K(keys).transpose(2,1) * \n",
        "                                  self.scaling_factor)\n",
        "        # Causal mask\n",
        "        mask = torch.triu(torch.full_like(unnormalized_attention, \n",
        "                                          fill_value=self.neg_inf), \n",
        "                          diagonal=+1)\n",
        "        unnormalized_attention += mask\n",
        "        # assert unnormalized_attention.shape == (batch_size, k, seq_len)\n",
        "        # Softmax over seq_len and dim = 1\n",
        "        attention_weights = self.softmax(unnormalized_attention.transpose(2,1))\n",
        "        # assert attention_weights.shape == (batch_size, seq_len, k)\n",
        "        context = attention_weights.transpose(2, 1) @ self.V(values)\n",
        "        # assert context.shape == (batch_size, k, hidden_size)\n",
        "        return context, attention_weights\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ece07c-d3b6-4255-d979-2c4f02ca0b62"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 100,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stand', 'andstay')\n",
            "('taverns', 'avernstay')\n",
            "('ebullition', 'ebullitionway')\n",
            "('subject', 'ubjectsay')\n",
            "('tranquil', 'anquiltray')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 3.262 | Val loss: 2.492 | Gen: etetetetetay isy ilay ay ay\n",
            "Epoch:   1 | Train loss: 2.279 | Val loss: 2.145 | Gen: etetetetetay isway innnntintintintintin isway ionway\n",
            "Epoch:   2 | Train loss: 1.994 | Val loss: 1.918 | Gen: etay iway innntiontintintintin isway onway\n",
            "Epoch:   3 | Train loss: 1.795 | Val loss: 1.783 | Gen: etay iway ontintintinay isway ongay\n",
            "Epoch:   4 | Train loss: 1.663 | Val loss: 1.731 | Gen: etay iway ontingay isway orgay\n",
            "Epoch:   5 | Train loss: 1.564 | Val loss: 1.708 | Gen: ethay iway ontingay isway ogay\n",
            "Epoch:   6 | Train loss: 1.482 | Val loss: 1.654 | Gen: ethay iway ontingititiongay away ogway\n",
            "Epoch:   7 | Train loss: 1.431 | Val loss: 1.545 | Gen: ethay iway ontioiongay issway orway\n",
            "Epoch:   8 | Train loss: 1.341 | Val loss: 1.504 | Gen: ethay iway ontioiongiongay isway oringway\n",
            "Epoch:   9 | Train loss: 1.263 | Val loss: 1.534 | Gen: ethay iway ontioiongiongay isway oingingway\n",
            "Epoch:  10 | Train loss: 1.211 | Val loss: 1.471 | Gen: ethay iway ontioiongiongay isway oringingway\n",
            "Epoch:  11 | Train loss: 1.168 | Val loss: 1.384 | Gen: ethay iway ongingingay isway oringway\n",
            "Epoch:  12 | Train loss: 1.119 | Val loss: 1.418 | Gen: eay iwayway ontingingway isway oringway\n",
            "Epoch:  13 | Train loss: 1.074 | Val loss: 1.398 | Gen: ethay iway ontingingway isway oringway\n",
            "Epoch:  14 | Train loss: 1.022 | Val loss: 1.395 | Gen: ethay iway ondingititingway issway orifway\n",
            "Epoch:  15 | Train loss: 0.988 | Val loss: 1.342 | Gen: ethay arirway ondiongway issway orway\n",
            "Epoch:  16 | Train loss: 0.944 | Val loss: 1.334 | Gen: ethay arirway ondiongway issssssway orway\n",
            "Epoch:  17 | Train loss: 0.908 | Val loss: 1.318 | Gen: ethay arway ondiongway issway orway\n",
            "Epoch:  18 | Train loss: 0.876 | Val loss: 1.295 | Gen: ethay ariway ondiongway issway orway\n",
            "Epoch:  19 | Train loss: 0.837 | Val loss: 1.257 | Gen: ethay ariway ondingiongway issway orway\n",
            "Epoch:  20 | Train loss: 0.810 | Val loss: 1.262 | Gen: ethay ariray ondinciongway issway orway\n",
            "Epoch:  21 | Train loss: 0.775 | Val loss: 1.342 | Gen: ethay ariray ondintioncatindiongn issway orrkingway\n",
            "Epoch:  22 | Train loss: 0.759 | Val loss: 1.373 | Gen: ethay arirway ondinditiongndingind issway orifray\n",
            "Epoch:  23 | Train loss: 0.749 | Val loss: 1.304 | Gen: ethay ariway ondindincingay issway orkingway\n",
            "Epoch:  24 | Train loss: 0.744 | Val loss: 1.359 | Gen: ethay arriway ondintintincay issway orkingray\n",
            "Epoch:  25 | Train loss: 0.711 | Val loss: 1.258 | Gen: ethay arrway ondincingcay issway orkingway\n",
            "Epoch:  26 | Train loss: 0.714 | Val loss: 1.187 | Gen: ethay arriway onditioncay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.665 | Val loss: 1.179 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.623 | Val loss: 1.182 | Gen: ethay arway ondionditay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.596 | Val loss: 1.151 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.573 | Val loss: 1.099 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.553 | Val loss: 1.095 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.541 | Val loss: 1.106 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.537 | Val loss: 1.061 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.520 | Val loss: 1.069 | Gen: ethay arway ondinditiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.501 | Val loss: 1.048 | Gen: ethay arway ondidiodcatintiontic isway okikay\n",
            "Epoch:  36 | Train loss: 0.487 | Val loss: 1.102 | Gen: ethay arway ondindiodcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.524 | Val loss: 1.009 | Gen: ethay arway ondidiodcationtioion isway okingway\n",
            "Epoch:  38 | Train loss: 0.473 | Val loss: 1.067 | Gen: ethay arriray ondidiodcationtiongc isway okikingway\n",
            "Epoch:  39 | Train loss: 0.434 | Val loss: 0.991 | Gen: ethay arriway ondidiongionationgit isway okikingway\n",
            "Epoch:  40 | Train loss: 0.411 | Val loss: 0.988 | Gen: ethay arriray ondindiodiongcationt isway okikingway\n",
            "Epoch:  41 | Train loss: 0.393 | Val loss: 0.953 | Gen: ethay arriway ondidingiongcationti isway okingray\n",
            "Epoch:  42 | Train loss: 0.379 | Val loss: 0.962 | Gen: ethay arriway ondindioingcay isway okingray\n",
            "Epoch:  43 | Train loss: 0.366 | Val loss: 0.947 | Gen: ethay arriway ondindioingcay isway okingsway\n",
            "Epoch:  44 | Train loss: 0.356 | Val loss: 0.951 | Gen: ethay arriway ondidingiongcay isway okingway\n",
            "Epoch:  45 | Train loss: 0.344 | Val loss: 0.930 | Gen: ethay arraway ondidingiongcay isway okingway\n",
            "Epoch:  46 | Train loss: 0.334 | Val loss: 0.974 | Gen: ethay arriway ondidingiongcationti iswiway okingway\n",
            "Epoch:  47 | Train loss: 0.329 | Val loss: 0.926 | Gen: ethay arriway ondidingcationay isway okingway\n",
            "Epoch:  48 | Train loss: 0.339 | Val loss: 0.986 | Gen: ethay arriway ondidingiongcationti isway okingsway\n",
            "Epoch:  49 | Train loss: 0.339 | Val loss: 0.946 | Gen: ethay arriway ondidingiongcay isway okingway\n",
            "Epoch:  50 | Train loss: 0.319 | Val loss: 0.968 | Gen: ethay arriway ondidingiongcay iswiway orkingway\n",
            "Epoch:  51 | Train loss: 0.360 | Val loss: 0.937 | Gen: ehay arriway ondidingiongcay isway okingway\n",
            "Epoch:  52 | Train loss: 0.351 | Val loss: 0.965 | Gen: ethay arriway ondidiongionay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.341 | Val loss: 0.896 | Gen: ethay arrway ondidingcationay isway okingway\n",
            "Epoch:  54 | Train loss: 0.311 | Val loss: 0.909 | Gen: ethay arraway ondindiongcay isway okingway\n",
            "Epoch:  55 | Train loss: 0.278 | Val loss: 0.873 | Gen: ehay arrway ondidingcay isway okingway\n",
            "Epoch:  56 | Train loss: 0.263 | Val loss: 0.837 | Gen: ehay arrway ondidingcationay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.247 | Val loss: 0.844 | Gen: ehay arrway ondindiongcay isway okingway\n",
            "Epoch:  58 | Train loss: 0.237 | Val loss: 0.850 | Gen: ehay arrway ondidingiongcay isway okingway\n",
            "Epoch:  59 | Train loss: 0.229 | Val loss: 0.851 | Gen: eethay arrway ondidingcationay isway okingway\n",
            "Epoch:  60 | Train loss: 0.222 | Val loss: 0.854 | Gen: ehay arrway ondidingcationay isway okingway\n",
            "Epoch:  61 | Train loss: 0.216 | Val loss: 0.853 | Gen: eethay arrway ondidingiongcay isway okingway\n",
            "Epoch:  62 | Train loss: 0.209 | Val loss: 0.860 | Gen: ethay arrway ondidingcationay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.203 | Val loss: 0.856 | Gen: eethay arirway ondidingiongcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.197 | Val loss: 0.863 | Gen: ethay arirway ondidingiongcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.191 | Val loss: 0.858 | Gen: eethay arirway ondidingiongcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.186 | Val loss: 0.865 | Gen: ethay ariway ondidingiongcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.180 | Val loss: 0.863 | Gen: ethay arirway ondidiongcationgiay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.174 | Val loss: 0.871 | Gen: ethay ariway ondidingcationtiongs isway orkingway\n",
            "Epoch:  69 | Train loss: 0.169 | Val loss: 0.872 | Gen: ethay ariway ondidiongcatinay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.164 | Val loss: 0.885 | Gen: ethay ariway ondidiongcatiay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.159 | Val loss: 0.875 | Gen: ethay ariway ondidiongcatinay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.156 | Val loss: 0.914 | Gen: ethay arrway ondidiongcatiay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.155 | Val loss: 0.897 | Gen: ethay ariway ondidiongcatintiay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.151 | Val loss: 0.924 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.144 | Val loss: 0.901 | Gen: ethay arrway ondidingcationgcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.137 | Val loss: 0.929 | Gen: ethay arrway ondidingcationgcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.132 | Val loss: 0.924 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.129 | Val loss: 0.932 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.124 | Val loss: 0.930 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.121 | Val loss: 0.936 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.118 | Val loss: 0.950 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.120 | Val loss: 0.965 | Gen: ethay arrway ondidiongcatinay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.131 | Val loss: 1.151 | Gen: eethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.217 | Val loss: 1.171 | Gen: ethay arway onditiongiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.331 | Val loss: 1.019 | Gen: etay arway ondidingitinay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.220 | Val loss: 0.871 | Gen: ethay arirway ondidiongitiay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.151 | Val loss: 0.937 | Gen: ethay arirway ondidiongcatinay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.132 | Val loss: 0.900 | Gen: ethay ariway ondidiongcatinay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.119 | Val loss: 0.880 | Gen: ethay arirway onditiongcatinay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.112 | Val loss: 0.879 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.106 | Val loss: 0.881 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.102 | Val loss: 0.885 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.099 | Val loss: 0.890 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.095 | Val loss: 0.896 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.092 | Val loss: 0.902 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.090 | Val loss: 0.909 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.087 | Val loss: 0.916 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.084 | Val loss: 0.923 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.082 | Val loss: 0.930 | Gen: ethay arrway onditiongcatinay isway orkingway\n",
            "Obtained lowest validation loss of: 0.837032363191247\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arrway onditiongcatinay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56bd7c80-7f4a-4d5a-f65d-632f6b3dc826"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arrway onditiongcatinay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f741b2c9-5186-482a-8ba1-e4bf06f21d96"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':100,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 10,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('subcommittee', 'ubcommitteesay')\n",
            "('subject', 'ubjectsay')\n",
            "('density', 'ensityday')\n",
            "('snowmobile', 'owmobilesnay')\n",
            "('aerial', 'aerialway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.868 | Val loss: 2.380 | Gen: eeew oday onnnnotonnnnnnnnnnnn edy onnnltotonny\n",
            "Epoch:   1 | Train loss: 2.138 | Val loss: 2.115 | Gen: eeay ay onnononononononnnono isy-ay onnditay\n",
            "Epoch:   2 | Train loss: 1.886 | Val loss: 1.984 | Gen: etay-ay-ay aiay-ay onginiononayay-onaya isy-isay onginingingingingaya\n",
            "Epoch:   3 | Train loss: 1.713 | Val loss: 1.837 | Gen: etetay aiway ongininingay isy ongingingay\n",
            "Epoch:   4 | Train loss: 1.574 | Val loss: 1.763 | Gen: etay aiway oningionangininganan isay ongingingay\n",
            "Epoch:   5 | Train loss: 1.478 | Val loss: 1.694 | Gen: etetay iatiway oninginingay isisgisay ongegingay\n",
            "Epoch:   6 | Train loss: 1.442 | Val loss: 1.686 | Gen: etay aiway oninioningay-onay-ay isway orartingedgay\n",
            "Epoch:   7 | Train loss: 1.338 | Val loss: 1.606 | Gen: etay iaway oonginingtingay isy-isway oongingedgay\n",
            "Epoch:   8 | Train loss: 1.266 | Val loss: 1.604 | Gen: etay iaway ootinininginingay-on isway ooringingay\n",
            "Epoch:   9 | Train loss: 1.230 | Val loss: 1.532 | Gen: ettway iway ontingway istisway origrway\n",
            "Epoch:  10 | Train loss: 1.216 | Val loss: 1.559 | Gen: tay aiway oionininginingway iway oorigingingway\n",
            "Epoch:  11 | Train loss: 1.128 | Val loss: 1.355 | Gen: ethay aiway ontingcay isway ongingway\n",
            "Epoch:  12 | Train loss: 1.068 | Val loss: 1.354 | Gen: tetay aiway ontiningtingway isway origingway\n",
            "Epoch:  13 | Train loss: 1.031 | Val loss: 1.259 | Gen: ethtway iarway ontiningway isway orengingway\n",
            "Epoch:  14 | Train loss: 1.012 | Val loss: 1.294 | Gen: tetway aiway ontiningctingway isway ongingway\n",
            "Epoch:  15 | Train loss: 0.957 | Val loss: 1.338 | Gen: eththay irarway ontingingway issway ortingway\n",
            "Epoch:  16 | Train loss: 0.921 | Val loss: 1.340 | Gen: tethay irarway ontiningingctingway issway orkgingrway\n",
            "Epoch:  17 | Train loss: 0.878 | Val loss: 1.286 | Gen: eththay irarway ontiningway issway orkigrway\n",
            "Epoch:  18 | Train loss: 0.857 | Val loss: 1.137 | Gen: ethay iwarway ontioningingway isway orkigrway\n",
            "Epoch:  19 | Train loss: 0.799 | Val loss: 1.126 | Gen: ethway iray ontiningway isway orkigrway\n",
            "Epoch:  20 | Train loss: 0.775 | Val loss: 1.065 | Gen: ethay iway ondiningingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.763 | Val loss: 1.152 | Gen: etheway irarway ondiningcingway isway orkigrway\n",
            "Epoch:  22 | Train loss: 0.755 | Val loss: 1.053 | Gen: ethay iray ondiiningingway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.710 | Val loss: 1.088 | Gen: tetway irarway ondiningcingway isway orkigrway\n",
            "Epoch:  24 | Train loss: 0.704 | Val loss: 1.002 | Gen: ethay iray ondiingingingway isway orkigrway\n",
            "Epoch:  25 | Train loss: 0.673 | Val loss: 1.022 | Gen: ethay irayway ondiningcingway isway orkigrway\n",
            "Epoch:  26 | Train loss: 0.640 | Val loss: 0.944 | Gen: ethay irarway ondiinigcingway isway orkigrway\n",
            "Epoch:  27 | Train loss: 0.610 | Val loss: 0.953 | Gen: ethay iray ondiningcingway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.594 | Val loss: 0.936 | Gen: ethay iraway ondiningcingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.577 | Val loss: 0.945 | Gen: ethay arirway ondiningcingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.566 | Val loss: 0.916 | Gen: ethay airway ondiningciningway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.539 | Val loss: 0.963 | Gen: ethay ariway ondiningcingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.533 | Val loss: 0.911 | Gen: ethay airway ondininicingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.520 | Val loss: 0.915 | Gen: ethay arirway ondiningcingway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.509 | Val loss: 0.956 | Gen: ethay iraway ondiningciongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.490 | Val loss: 0.921 | Gen: ethay ariway ondiningcingcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.469 | Val loss: 0.871 | Gen: ethay ariway ondiningcingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.449 | Val loss: 0.897 | Gen: ethay arirway ondiningcingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.445 | Val loss: 0.861 | Gen: ethay ariway ondiningicingcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.434 | Val loss: 0.845 | Gen: ethay arirway ondiningcingcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.427 | Val loss: 0.872 | Gen: ethay airway ondiningcingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.423 | Val loss: 0.817 | Gen: ethay ariway ondiningingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.401 | Val loss: 0.856 | Gen: ethay ariway ondiiningcingcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.396 | Val loss: 0.781 | Gen: ethay arirway ondintingcingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.367 | Val loss: 0.762 | Gen: ethay ariway ondintingcingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.356 | Val loss: 0.767 | Gen: ethay arirway ondintingcingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.347 | Val loss: 0.745 | Gen: ethay ariway ondintingcingcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.337 | Val loss: 0.766 | Gen: ethay arirway ondintingcingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.334 | Val loss: 0.739 | Gen: ethay irway ondintingcingcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.316 | Val loss: 0.737 | Gen: ethay irway ondintingcingcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.307 | Val loss: 0.717 | Gen: ethay irway onditingcingcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.293 | Val loss: 0.732 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.288 | Val loss: 0.695 | Gen: ethay irway onditingcingcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.276 | Val loss: 0.727 | Gen: ethay irway onditingcingcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.269 | Val loss: 0.743 | Gen: ethay irway onditingcingcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.258 | Val loss: 0.741 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.251 | Val loss: 0.723 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.242 | Val loss: 0.702 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.233 | Val loss: 0.688 | Gen: ethay irway onditingcingcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.226 | Val loss: 0.672 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.218 | Val loss: 0.682 | Gen: ethay irwaway onditingcingcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.211 | Val loss: 0.665 | Gen: ethay irwaway onditingingcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.204 | Val loss: 0.665 | Gen: ethay irwaway onditingingcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.199 | Val loss: 0.626 | Gen: ethay irwaway onditingingcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.221 | Val loss: 0.695 | Gen: ethay irwaway onditiongingcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.272 | Val loss: 0.823 | Gen: ethay irwaway onditingingcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.301 | Val loss: 0.698 | Gen: ethay irrway onditingingcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.233 | Val loss: 0.621 | Gen: ethay iriray onditingingcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.202 | Val loss: 0.628 | Gen: ethay irwaway onditingingcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.187 | Val loss: 0.558 | Gen: ethay iriray onditingingcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.174 | Val loss: 0.553 | Gen: ethay iriray onditingingcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.165 | Val loss: 0.537 | Gen: ethay iriray onditingingcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.160 | Val loss: 0.529 | Gen: ethay iriray onditiongcingcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.154 | Val loss: 0.521 | Gen: ethay iriray onditiongcingcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.150 | Val loss: 0.517 | Gen: ethay iriray onditiongcincay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.145 | Val loss: 0.513 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.141 | Val loss: 0.509 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.137 | Val loss: 0.507 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.133 | Val loss: 0.505 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.129 | Val loss: 0.503 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.126 | Val loss: 0.501 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.122 | Val loss: 0.499 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.119 | Val loss: 0.498 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.116 | Val loss: 0.496 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.112 | Val loss: 0.495 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.109 | Val loss: 0.493 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.106 | Val loss: 0.495 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.104 | Val loss: 0.489 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.101 | Val loss: 0.494 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.098 | Val loss: 0.487 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.095 | Val loss: 0.495 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.093 | Val loss: 0.501 | Gen: ethay irwaway onditiongcincay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.114 | Val loss: 0.654 | Gen: ethatay iraway onditiongcinccay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.218 | Val loss: 0.811 | Gen: ethay iriway ondiitiningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.212 | Val loss: 0.597 | Gen: ethay arirway onditiongingcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.169 | Val loss: 0.661 | Gen: ethay irwaway onditioningcccay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.145 | Val loss: 0.500 | Gen: ethay iriway onditioncingcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.109 | Val loss: 0.443 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.095 | Val loss: 0.445 | Gen: ethay iriway onditiongcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.090 | Val loss: 0.439 | Gen: ethay iriway onditiongcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.43923975067775484\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay iriway onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af807cfe-da2b-4bd2-cc69-a7deee881c2a"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stand', 'andstay')\n",
            "('taverns', 'avernstay')\n",
            "('ebullition', 'ebullitionway')\n",
            "('subject', 'ubjectsay')\n",
            "('tranquil', 'anquiltray')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.504 | Val loss: 1.923 | Gen: etay irway ongway isssssssway orway\n",
            "Epoch:   1 | Train loss: 1.712 | Val loss: 1.627 | Gen: ettay aiway ontingay isssssssssssssssway odododay\n",
            "Epoch:   2 | Train loss: 1.453 | Val loss: 1.393 | Gen: ethethhhhethethhhhhh ariray onday-ingatingngway issssssissssssssway orgingway\n",
            "Epoch:   3 | Train loss: 1.251 | Val loss: 1.344 | Gen: eteteway aiririray oicationgionay isway origway\n",
            "Epoch:   4 | Train loss: 1.121 | Val loss: 1.296 | Gen: ethetay airiway oicationay isisisay oioway-ingay\n",
            "Epoch:   5 | Train loss: 0.984 | Val loss: 1.264 | Gen: ethay aiway onditiongiongway isisiway oioigway\n",
            "Epoch:   6 | Train loss: 0.868 | Val loss: 1.102 | Gen: ehethay aiway ondintingway isway oray-ingway\n",
            "Epoch:   7 | Train loss: 0.747 | Val loss: 1.069 | Gen: ehetay arwayway odcindingitingway isway orkingway\n",
            "Epoch:   8 | Train loss: 0.636 | Val loss: 0.874 | Gen: ethay aririway onditiondiontionay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.545 | Val loss: 0.983 | Gen: ethay airway onditingiongnditiong isway orkingway\n",
            "Epoch:  10 | Train loss: 0.504 | Val loss: 0.877 | Gen: etetay arirway ondintingingngday isway okingngway\n",
            "Epoch:  11 | Train loss: 0.412 | Val loss: 1.037 | Gen: ehthay airway ondidiiiitininiingca isway orkingway\n",
            "Epoch:  12 | Train loss: 0.368 | Val loss: 0.799 | Gen: ethay arirwaywway onditingitingngcay isway okingray\n",
            "Epoch:  13 | Train loss: 0.339 | Val loss: 0.837 | Gen: ethay arwaywwayway onditingitingway isway orkingway\n",
            "Epoch:  14 | Train loss: 0.304 | Val loss: 0.822 | Gen: ethay irwaywayway odidiiinitingcay isway oikingway\n",
            "Epoch:  15 | Train loss: 0.283 | Val loss: 0.708 | Gen: ehthay airway onditioniay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.250 | Val loss: 0.659 | Gen: ehtay airway onditiongiongcay isway okingway\n",
            "Epoch:  17 | Train loss: 0.210 | Val loss: 0.615 | Gen: etethay airway onditioniongingcay isiway orkingway\n",
            "Epoch:  18 | Train loss: 0.184 | Val loss: 0.652 | Gen: ethay airway onditioniongicay iway orkingway\n",
            "Epoch:  19 | Train loss: 0.165 | Val loss: 0.638 | Gen: ehthay airway onditiongiongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.164 | Val loss: 0.747 | Gen: ethay airway onditiongiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.223 | Val loss: 0.666 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.189 | Val loss: 0.613 | Gen: ethay airway onditingingingcay isway owingway\n",
            "Epoch:  23 | Train loss: 0.193 | Val loss: 0.638 | Gen: ethay airway onditiongingngway isway oringway\n",
            "Epoch:  24 | Train loss: 0.159 | Val loss: 0.584 | Gen: eteay airway onditioniongdcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.100 | Val loss: 0.466 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.076 | Val loss: 0.472 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.064 | Val loss: 0.424 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.051 | Val loss: 0.450 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.043 | Val loss: 0.451 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.035 | Val loss: 0.462 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.034 | Val loss: 0.471 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.032 | Val loss: 0.601 | Gen: ethay arwway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.061 | Val loss: 0.657 | Gen: etethay arway onditioncingcay iway orkngway\n",
            "Epoch:  34 | Train loss: 0.108 | Val loss: 0.690 | Gen: ethay airway onditioningingcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.139 | Val loss: 0.771 | Gen: ethay irway onditioniniongway isway owingway\n",
            "Epoch:  36 | Train loss: 0.123 | Val loss: 0.548 | Gen: ehthay airway onditioningingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.062 | Val loss: 0.431 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.036 | Val loss: 0.455 | Gen: ethay arway onditiongingcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.039 | Val loss: 0.455 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.027 | Val loss: 0.418 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.019 | Val loss: 0.444 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.016 | Val loss: 0.440 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.014 | Val loss: 0.445 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.012 | Val loss: 0.428 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.009 | Val loss: 0.464 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.008 | Val loss: 0.438 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.008 | Val loss: 0.481 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.007 | Val loss: 0.453 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.006 | Val loss: 0.493 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.41776718463079304\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8574d9-4620-47fc-c042-dfc091680f3c"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('subcommittee', 'ubcommitteesay')\n",
            "('subject', 'ubjectsay')\n",
            "('density', 'ensityday')\n",
            "('snowmobile', 'owmobilesnay')\n",
            "('aerial', 'aerialway')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.794 | Val loss: 2.196 | Gen: oooomay ay intinayayay ay onay\n",
            "Epoch:   1 | Train loss: 1.871 | Val loss: 1.924 | Gen: ay ay indinay isway ingway\n",
            "Epoch:   2 | Train loss: 1.604 | Val loss: 1.795 | Gen: ay ay ontingtinay ay onay\n",
            "Epoch:   3 | Train loss: 1.461 | Val loss: 1.774 | Gen: eray-y-y aywayway ondingay isway orway\n",
            "Epoch:   4 | Train loss: 1.340 | Val loss: 1.568 | Gen: ethay iway ondidingay isay orgray\n",
            "Epoch:   5 | Train loss: 1.213 | Val loss: 1.490 | Gen: etheway arrrararay ondiday isway ororgay\n",
            "Epoch:   6 | Train loss: 1.136 | Val loss: 1.357 | Gen: etheay iway ondiotingndinay isway orfingway\n",
            "Epoch:   7 | Train loss: 1.050 | Val loss: 1.296 | Gen: ethehay iwaywayaywayayayway odingonay iswaywayay orgway\n",
            "Epoch:   8 | Train loss: 0.939 | Val loss: 1.210 | Gen: ethay iway ondiotinininay isway ortingway\n",
            "Epoch:   9 | Train loss: 0.803 | Val loss: 1.083 | Gen: ethay iwaway onditingnay isway orkingway\n",
            "Epoch:  10 | Train loss: 0.731 | Val loss: 1.020 | Gen: ethay arway onditingnioiway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.657 | Val loss: 1.069 | Gen: ethay iwaway onditiningcay isway orringway\n",
            "Epoch:  12 | Train loss: 0.601 | Val loss: 0.986 | Gen: ethay arway onditingcay isway orgkngway\n",
            "Epoch:  13 | Train loss: 0.581 | Val loss: 0.917 | Gen: ethay arway ondiniongcinay isay orvingway\n",
            "Epoch:  14 | Train loss: 0.535 | Val loss: 0.913 | Gen: ethay arway onditiningniay iway orkingway\n",
            "Epoch:  15 | Train loss: 0.475 | Val loss: 0.847 | Gen: ethay arway onditiningcay iway orrkingway\n",
            "Epoch:  16 | Train loss: 0.428 | Val loss: 0.776 | Gen: ehtay arway onditingcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.386 | Val loss: 0.783 | Gen: ethay arway onditinongcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.338 | Val loss: 0.856 | Gen: ehtway arway onditingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.329 | Val loss: 0.743 | Gen: ehtay arway onditingconay isay orkingway\n",
            "Epoch:  20 | Train loss: 0.296 | Val loss: 0.774 | Gen: ethay aiway onditiningcanway iway orkingway\n",
            "Epoch:  21 | Train loss: 0.286 | Val loss: 0.796 | Gen: eehtay airway onditinongcay isay orkingway\n",
            "Epoch:  22 | Train loss: 0.269 | Val loss: 0.697 | Gen: ehthay ariway onditiningcangcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.232 | Val loss: 0.776 | Gen: ehthay ariway onditiniongcay isay orkingway\n",
            "Epoch:  24 | Train loss: 0.219 | Val loss: 0.702 | Gen: ehtay aiway onditioningcay isay orkingway\n",
            "Epoch:  25 | Train loss: 0.226 | Val loss: 0.712 | Gen: ehtway airway onditingcingtinay isway orkinggway\n",
            "Epoch:  26 | Train loss: 0.228 | Val loss: 1.035 | Gen: ehthway ariwwarwaywaywayway onditiningcinay iswaysway orkingnjway\n",
            "Epoch:  27 | Train loss: 0.299 | Val loss: 0.971 | Gen: etay aryway onditioningcay iway orrkingway\n",
            "Epoch:  28 | Train loss: 0.308 | Val loss: 0.730 | Gen: eehtay ariway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.225 | Val loss: 0.818 | Gen: ehetay ariway onditiningnangcay isway orkingnjay\n",
            "Epoch:  30 | Train loss: 0.212 | Val loss: 0.553 | Gen: ehthay ariway onditiongingcay isway orkinggway\n",
            "Epoch:  31 | Train loss: 0.149 | Val loss: 0.713 | Gen: eethay aiway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.156 | Val loss: 0.523 | Gen: ehthay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.107 | Val loss: 0.509 | Gen: ehetay ariway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.093 | Val loss: 0.526 | Gen: ehetay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.084 | Val loss: 0.483 | Gen: eethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.075 | Val loss: 0.496 | Gen: ehetay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.070 | Val loss: 0.475 | Gen: eethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.063 | Val loss: 0.483 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.060 | Val loss: 0.612 | Gen: eethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.082 | Val loss: 0.634 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.148 | Val loss: 0.599 | Gen: ehtay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.172 | Val loss: 0.664 | Gen: ehthay airwayway onditioniongncay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.152 | Val loss: 0.541 | Gen: ethtay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.097 | Val loss: 0.508 | Gen: ethay airway onditiongingcay isway orkinggway\n",
            "Epoch:  45 | Train loss: 0.077 | Val loss: 0.478 | Gen: eethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.053 | Val loss: 0.470 | Gen: eethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.049 | Val loss: 0.511 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.047 | Val loss: 0.485 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.044 | Val loss: 0.459 | Gen: ethay airway ondidioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.45930340046607804\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondidioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ee43b3a-b362-4169-c889-74d069afbd44"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "8676a098-36d8-4933-cc26-800b85428656"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "e72ba2a2-8ca6-4c13-b55c-de715ad84c9e"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l, )"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c+3myRAQjqBQBK6I6AGFcWR3RVxAaPOD1xQwdGfjI5xwwWSQAiEJUAgJOIyA0pw+KEzKuMoOFEzBgdBHAFNwh4kGkGkm7AESCAJZOl+fn/c21A03V3LuU3fDt93XvVK3dunnvt0VXU9de5yjiICMzOzejUNdgJmZjY0uYCYmVlDXEDMzKwhLiBmZtYQFxAzM2uIC4iZmTXEBWSIkjRG0ucHO4+y8fNi9sJxARm6xgD+oHy+Uj4vyvjvzbYppX9DS/qYpD9IulXSJZKanQsA5wMvy3OZP1hJSBop6ReSbpN0p6SPDFYuuVI8LwCS9pS0UtL3gDuBSYOYy08lLZe0QtLUQcxjjqSvVCyfK+nLg5WPpVGZr0SX9CrgAuADEbFF0sXATRHxvRdzLnk+ewI/j4jXDMb2K/L4IDAlIj6dL7dExLpBzGdPSvC8wDO53AO8MSJuGuRcdo6IxyTtACwF3hoRjw5CHnsCV0bE/nmP7M/AwYORi6XbbrATqOIdwAHAUkkAOwAPO5dSuQP4qqR5ZB/cvx3shErmvsEuHrkvSXp/fn8SMBl4wT+0I+Kvkh6VtB8wHrjFxWPoKnsBEfDdiDhlsBOhXLmURkT8SdL+wHuAcyRdExFzBjuvEtkw2AlIOgx4J/CGiNgo6Tpg+0FM6TvAccAE4LJBzMMSlf0YyDXA0ZJ2g6wbLmkP5wLAk8BOg7h9ACTtDmyMiH8H5gP7D3JKpXheSqYFeDwvHq8EXj/I+VwFTAEOApYMci6WoNQ9kIi4S9JpwNX5/tItwBeA+17MueT5PCrpd5LuBP47ImYMRh7AvsB8SV1kz8nnBikPoFTPS5n8EvispD8CK4FB3aUWEZslXQusjYjOwczF0pT6ILqZbXvyL2A3Ax+KiD8Pdj7WuLLvwjKzbYikfYBVwDUuHkOfeyBmZtYQ90DMzKwhLiBmZtaQIVNABnP4hZ6cy/OVJQ9wLn1xLr0rUy5DzZApIECZXmTn8nxlyQOcS1+cS+/KlMuQMpQKiJmZlciAX0jY1NQczc3pm2lpGcOwYSOSThkrIg+AMWPGsv32I5Ny6ezcWkguRTwvXV1dBeTRQnPzsORT+rq60p+XlpYWJCXnUsTo6y0tLTQ1NSfnElHMa1TE89LUlD4IdfZ+2a4Up4AWlUtXV+eaiNi1iJymTJkSa9asqbn98uXLl0TElCK2XY8BLyDNzduxyy67J8eZNetE5s69MCnG6NHjkvMA+NKXPsk3v5k2hM+TTz5WSC6nnHIC5533taQYGzc+kZzHmWeexhlnnJMc54kn0sfVmz17NtOnp1+APnx4+nBRp59+BrNmnZYcZ/Pmp5NjzJ49mxkzTkqOM2rkmOQYZ54xmzPOODs5TmcBXzjOOONMZs8+MznOhg3rChuVYs2aNSxbtqzm9pKK+XCrU6mHMjEze7EaCtfouYCYmZVQlwuImZnVK3APxMzMGhIELiBmZlavgK7y1w8XEDOzsgmgs4DT6weaC4iZWQn5GIiZmTXEBcTMzOoWET6N18zMGuMeiJmZNcSn8ZqZWd2CoXEar4dzNzMroYio+VYLSVMkrZS0StLMXn7+EknXSrpF0u2S3lMtpguImVkJdeUH0mu5VSOpGbgIeDewD3CspH16NDsN+FFE7AccA1xcLW5NBUTShyTtlN8/TdKVkvav5bFmZlanOnofNfZADgZWRcQ9EbEZuAI4qudWgdH5/RbggWpBa+2BzI6IJyW9GXgn8K/At2p8rJmZ1aF7MMU6Csg4Scsqbj2n6W0F7q9Ybs/XVToT+JikdmAx8MVqeaqW6iXplojYT9J5wB0R8YPudX20n0o+z3BLy5gD5syZU3Ub1UyYMJ4HH3woKUZTUzHnDIwfP46HHqp9trDedHV1FpJLEc9LEbnsvvtEHnhgdXKcImZqbGtro729PTlOETMStra20tHRkRyniBkJi3pemgv4O9q9dSIPdKS/X4o4U6m1dXc6Oqp+2a7qxBNPWB4RByYHAv5uv/3il9deW3P73ceO7Xfbko4GpkTEP+XLHwcOiYjjK9qcSFYTvirpDWQdhddEP2++Wt8JHZIuAQ4H5kkaQT+9l4hYCCwEGDZsRKTOJAiekbAvZZmR8KyzyjMj4YIF8wuZkXDEiB2SY8yde05pZiScP/+CQmYk3GnU2OQYZ51VnhkJzz67mBkJi1bwdSAdwKSK5bZ8XaVPAVPybd8oaXtgHPBwX0Fr/Yr1YWAJ8K6IWAvsDKT/hZqZWS+irn81WApMlrSXpOFkB8kX9WjzN+AdAJJeBWwPPNJf0Jp6IBGxEbiyYnk1kN7/NDOz54mCh3OPiK2SjifrCDQDl0XECklzgGURsQiYBlwq6QSywzDHRZVukC8kNDMroaKHMomIxWQHxyvXnV5x/y7gTfXEdAExMyshj4VlZmZ1y4YycQExM7MGuAdiZmb183wgZmbWKPdAzMysbgF0uoCYmVkj3AMxM7OGuICYmVndwgfRzcysUe6BmJlZQ1xAzMysbr4SPbd162YeeeT+6g2r2LKlmDhF6OrayhNPpE0oNaqAORUAmpqaGTmyJSnG44+lD6zc1dXFpqc3JMeRlByjqDibNj2VHCOiq5A4FDBxEhQzMdUTBcxl09m1tZA4w4YNT44R0VXIfCtFK2KyrIHmHoiZWQkVOZz7QHEBMTMrm2fnOi81FxAzs5IJfBDdzMwa5IPoZmbWEPdAzMysIS4gZmZWt6EylEnTYCdgZmbPF3X8q4WkKZJWSlolaWYvP/+apFvz258kra0W0z0QM7MSKvI6EEnNwEXA4UA7sFTSooi4q7tNRJxQ0f6LwH7V4roHYmZWMt2n8dZ6q8HBwKqIuCciNgNXAEf10/5Y4IfVgroHYmZWQnUeRB8naVnF8sKIWFix3ApUjgXVDhzSWyBJewB7Ab+utlEXEDOzEqrzIPqaiDiwoE0fA/w4IjqrNXQBMTMrm+KHMukAJlUst+XrenMM8IVagrqAmJmVTACdXekjJ1dYCkyWtBdZ4TgG+GjPRpJeCYwFbqwlaNWD6JLm1bLOzMyKU+RpvBGxFTgeWAL8EfhRRKyQNEfSkRVNjwGuiBq7P7X0QA4HTu6x7t29rDMzs4IUfR1hRCwGFvdYd3qP5TPridlnAZH0OeDzwEsl3V7xo52A39WzETMzq91QmZFQffVUJLWQ7Qs7D6i8avHJiOh3KjFJU4GpAC0tLQfMnj07OdG2tjba29uTYhQxexnAhAnjefDBh5JiNDUVc/hp/PhxPPRQ2uyIRczG1traSkdHX8fkalfEjHlFvFeK4lx6V1QuRcw8WdR7d9q0acuLOhNq8j77xNd/8IOa2//9fvsVtu169PkpFhHrgHVkF5TUJT//eCGApDjppPS9XRdcMI/UOLvuOql6oxrMmnUic+demBSjqCltv/KVT/P1r1+aFKP9/ruT85h73rnMOuXU5Dibt2xKjjF//gXMmHFScpwizoJZsGA+06fPSI5TxJS2CxYsYPr06QXkkv6hXdTzUsSXwvPPn8vMmbOS4xRtKPRAfBaWmVnJeEIpMzNrmAuImZk1xLuwzMysAbUP0z6YXEDMzEomovjrQAaCC4iZWQl5F5aZmTXEB9HNzKxuQ+VKdBcQM7MScg/EzMzqV/x8IAPCBcTMrIxcQMzMrBHR5QJiZmYNGAIdEBcQM7OyyS4kLH8FcQExMyshFxBg7NjxHP6uT6TH2XkCR394WlKMUWNHJecB0DJmHO/9wCeTYjz15FOF5DJy5GgOfsOUpBgPP3xfch5SE8NH7JAcZ2vnluQYIJqampKjdHWlT24FxUx6NBQ+TAbDlgLmj4mIQuIUa2ichZX+V2ZmZoWKgK7OrppvtZA0RdJKSaskzeyjzYcl3SVphaSqUyJ6F5aZWQkV2QOR1AxcBBwOtANLJS2KiLsq2kwGTgHeFBGPS9qtWlz3QMzMyqh7SN5abtUdDKyKiHsiYjNwBXBUjzafBi6KiMezzcfD1YK6gJiZlVCd9WOcpGUVt6k9wrUC91cst+frKu0N7C3pd5JuklT14Kp3YZmZlU1EvRcSromIAxO3uh0wGTgMaAOul7RvRKzt6wHugZiZlVDk42HVcqtBBzCpYrktX1epHVgUEVsi4l7gT2QFpU8uIGZmJRMUXkCWApMl7SVpOHAMsKhHm5+S9T6QNI5sl9Y9/QX1LiwzsxIq8iysiNgq6XhgCdAMXBYRKyTNAZZFxKL8Z0dIugvoBGZExKP9xXUBMTMroaIvJIyIxcDiHutOr7gfwIn5rSYuIGZmZRMBHo3XzMwaMRSGMnEBMTMroSFQP1xAzMzKpvssrLJzATEzK5ttZT4QZWNRt0XE/dXamplZMYbClLZVLyTMT+1aXK2dmZkVpfaLCAezp1Lrleg3SzpoQDMxM7NnDIUColo2Lulu4OXAfcAGQGSdk9f20X4qMBVg5513OWD+gm8kJ9oyegfWPZE2i19zczEjt4waOYL1G9JmMKt1EphqRo/egScSn5e1a9ck59HaOpGOjtXJcbq6OpNjtLW10t7ec5ifRqT/Yba1tdHe3l5ALumcS++KymX69OnLCxjQEIBJL315TJu7oOb2Jxz7/sK2XY9aD6K/q56gEbEQWAiw884TYsnVt9eb1/MTOOK1pMYpakrbQ1//cq6/aVVSjKKmtD387a/mV79ekRTj5/91SXIeZ511OmecMSc5zsaNTyTHmDdvHieffHJynCKmtJ0//wJmzDgpOU5Eei4LFixg+vTpyXGy74+pucxn+vQZBeSSXuSLe14Kti0cRAeIiPRJs83MrGYFfGcYcD6N18yshLaJ03jNzOwFFlHILtSB5gJiZlYyvhLdzMwaE0PjQkIXEDOzMnIPxMzM6je4FwjWygXEzKyEhkD9cAExMysj90DMzKxuMUQOohczOJSZmRWq6MEUJU2RtFLSKkkze/n5cZIekXRrfvunajHdAzEzK6Eid2FJagYuAg4H2oGlkhZFxF09mv5HRBxfa1z3QMzMSqfw+UAOBlZFxD0RsRm4AjgqNUsXEDOzsom6d2GNk7Ss4ja1R8RWoHJW2fZ8XU8flHS7pB9LmlQtTe/CMjMro/oOoq8pYD6QnwE/jIhNkj4DfBd4e38PGPACMmzEMCbsNSE9zvD0OH++9e7kPAC27L8Hq/+aNgHN44+lT74EsOlNL+XeP9+ZFOPDHz8hOY+dd9mtkDg33/Cb5Bg77rgTr933sOQ47e0rk2Nst91wxu3S2xe9+jzx5KPJMaQmRozYMTnO5s1PJ8cAkNLnFZGK2YnS1NScHKOIydC6ZWNhFRYOoAOo7FG05eue3WZE5ZvsO8AF1YJ6F5aZWQkVfAxkKTBZ0l6ShgPHAIsqG0iaWLF4JPDHakG9C8vMrGwKnus8IrZKOh5YAjQDl0XECklzgGURsQj4kqQjga3AY8Bx1eK6gJiZlVDRFxJGxGJgcY91p1fcPwU4pZ6YLiBmZiXkoUzMzKxunlDKzMwaMwCnYQ0EFxAzs9LxfCBmZtagrk4XEDMzq1f4GIiZmTXAB9HNzKxhLiBmZtaAGBIzErqAmJmVjY+BmJlZw1xAzMysEUOgftQ2nLsyH5N0er78EkkHD2xqZmYvTt1nYRU4nPuAqHU+kIuBNwDH5stPkk3QbmZmRYtsNN5ab4NFtVQvSTdHxP6SbomI/fJ1t0XE3/XRfiowFWCXXcYd8LVvpNeakTsMY8NTW5JibHqqmJnUxowZydq1G5JibN26uZBcdtllDI8+ujYpxg47jkzOY9TIEazfsCk5zsYN65NjjBs3ljVrHk+OU8TMexMnjmf16oeS43R2bk2O0draSkdHR/WGVUR0Jcdoa2ujvT1tVs+iFJXL9OnTlxcwrSwA4ydOimM/Oa3m9t+Ye0Jh265HrcdAtkhqJutZIWlXoM93UUQsBBZC9kQsW5H+pj3w1a2kxilqStsPfOBNXHnl75JiFDWl7SeOO5LvXr6oesN+vPqA9L2RbznkZfz2939JjlPElLafnvphLl34o+Q4RUxpe+ppMzj3nPnJcYqY0nbu3HOYNeu05DhFFNb58y9gxoyTkuMUMS3uBRfM46STTk6OU7Rt6SysbwJXAbtJOhc4Gkh/J5qZWa+2mQISEd+XtBx4ByDgfRFRdb5cMzNr0LZSQAAi4m6gmH1AZmbWp4jip7QdCLWehWVmZi+giNpvtZA0RdJKSaskzeyn3QclhaSqB+V9IaGZWekUe31HfhLURcDhQDuwVNKiiLirR7udgC8Dv68lrnsgZmYlVPCFhAcDqyLinojYDFwBHNVLu7OBeUBNp9q5gJiZlU3UXUDGSVpWcZvaI2IrcH/Fcnu+7hmS9gcmRcQvak3Tu7DMzEomqPsg+pqUCwklNQEXAsfV8zgXEDOzEir4OpAOYFLFclu+rttOwGuA6/KLMycAiyQdGRHL+grqAmJmVjYRRFf6kDEVlgKTJe1FVjiOAT767OZiHTCue1nSdcD0/ooH+BiImVkpFXkab0RsBY4HlgB/BH4UESskzZF0ZKM5ugdiZlZCRV9IGBGLgcU91p3eR9vDaonpAmJmVjLd84GUnQuImVnZeE50MzNrzODONFirAS8gax97lJ9dcXlynMknfiY5zvon0ycZAnjXu17DsqX/nRRj+x1GFZLLli2b6HhgVVKMffY/qJBcinjDf/Azn0iOMXaXHQuJc8PP/jc5xsiRLRx08HuS46xYkZ7LsGEjmDBhr+Q4Dz54b3IMqYnhw7dPjtPV1VlALqK5Of2jsIhcKrmAmJlZQ4bCaLwuIGZmZZMdRR/sLKpyATEzK5khUj9cQMzMysjHQMzMrAE+C8vMzBoxRKa0dQExMysh90DMzKxuHsrEzMwa5gJiZmYNqHGc9kHmAmJmVjYBUeh8UgPDBcTMrIS8C8vMzBriAmJmZnXzWVhmZtaYbWlCKUkC/gF4aUTMkfQSYEJE/GFAszMze1EKorP8R9FVS5WT9C2gC3h7RLxK0ljg6ojodSYiSVOBqQBjxow54Oyzz01OdPz4XXnooUeSYnR2FjPhy8SJ41m9+qGkGE1NTYXkMn78bjz00MNJMUa37Jycx6iRI1i/YVNynBE7jEiPsV0Tm7am//GtX7c+OcaYMSNZu3ZDcpynnkrPpYi/IcgmMUvV2tpKR0dHcpxsZ085cpk2bdryiDgwORAwduz4OOywY2tu/9OffqPqtiVNAb4BNAPfiYjze/z8s8AXgE5gPTA1Iu7qL2atu7AOiYj9Jd0CEBGPSxreV+OIWAgsBBgxYse48MJLatxM30488TOkxilqRsJTT5vBuefMT4pR1IyEM2Z8gfnzL0qKcfjf1/5G7cuhr38519+UNjMiwEtfmz5j3st22ZG/PLoxOc4NP7shOcb73/9GrroqPU4RMxJOm/Z5vvrVi5PjFDEj4dy55zBr1mnJcYqYBfD88+cyc+as5DhFioJ3YUlqBi4CDgfagaWSFvUoED+IiG/n7Y8ELgSm9Be31gKyJU8g8uC7kvVIzMyscEEUeyHIwcCqiLgHQNIVwFHAMwUkIp6oaD+SGrp3tRaQbwJXAbtJOhc4Gkj/+mBmZr2qswcyTtKyiuWF+Z6gbq3A/RXL7cAhPYNI+gJwIjAceHu1jdZUQCLi+5KWA+8ABLwvIv5Yy2PNzKx+dRaQNUUcf4mIi4CLJH2UrJPwif7a13wab0TcDdydlp6ZmdWi4NN4O4BJFctt+bq+XAF8q1rQYk4FMjOzwkRkx0BqvdVgKTBZ0l75CVDHAIsqG0iaXLH4XuDP1YL6QkIzszIqsAcSEVslHQ8sITuN97KIWCFpDrAsIhYBx0t6J7AFeJwqu6/ABcTMrJSigGtcnhMvYjGwuMe60yvuf7nemC4gZmYltM0MZWJmZi8sFxAzM2tA4RcSDggXEDOzkil6KJOB4gJiZlZCLiBmZtYQFxAzM2tAFHodyEBxATEzK6EYAgOeu4CYmZWQd2EBmzc/xb333p4cZ9Om9DhSMUN/bd26mTWPps1gtvPOEwrJpaurk40b1yXFuPKHaRNSAey796xC4ux1477JMT499cP8ZOGPkuN0PFB1KKCqjjji1Sxb9svkOGvXps2ACbBly9OsXv2X5DhNTcV8bBTx99jcXMTftGhuHpYcpYiZGrv5LCwzM2tQuICYmVljipiud6C5gJiZlZB7IGZmVr/wabxmZtaAoPjh3AeCC4iZWQl5MEUzM2uAz8IyM7MGuYCYmVlDhkIBKebSbDMzK0x2ElZXzbdaSJoiaaWkVZJm9vLzEyXdJel2SddI2qNaTBcQM7PSyY6B1HqrRlIzcBHwbmAf4FhJ+/RodgtwYES8FvgxcEG1uC4gZmZl1H0tSC236g4GVkXEPRGxGbgCOOq5m4trI2JjvngT0FYtqAuImVkJRR3/gHGSllXcpvYI1wrcX7Hcnq/ry6eA/66Wow+im5mVUJ0H0ddExIFFbFfSx4ADgbdWa1u1ByJpXi3rzMysKFH0QfQOYFLFclu+7jkkvRM4FTgyIqqOT1/LLqzDe1n37hoeZ2ZmDeieD6Sog+jAUmCypL0kDQeOARZVNpC0H3AJWfF4uJag6mvjkj4HfB54KVA5C81OwO8i4mN9Bs32v00FaGlpOWD27Nm15NKvtrY22tvbk+MUoYhcttsufQIbgIkTJ7B69YNJMYo43Xz33SfwwANpeQCMGLFDcoxx48ayZs3jyXE2b346OcbEieNZvTp9MqjOzi3JMVpbW+noSJsILaMS5ZKuqFymTTtxeVG7kXbccXS84hUH19z+1luvqbptSe8Bvg40A5dFxLmS5gDLImKRpP8B9gVW5w/5W0Qc2W/MfgpICzAWOA+oPGf4yYh4rJZfKo9TyNUwCxYsYPr06UkxipqRcP78C5gx46SkGEXNSDh79smcfXbaHsXOzvR5B844YxZnnTU3Oc5eexYzI+GlJZmRcNasacyd+9XkOEXMSHjeeedyyimnJscpYkbCc8+dw6mnnp4cpwhF5fL00+sLLSB7731Qze1vu+3XhW27Hn2+EyJiHbAOOPaFS8fMzGBoXInus7DMzEonwKPxmplZIzwfiJmZ1a37LKyycwExMyudoKsr/eSWgeYCYmZWQu6BmJlZQ1xAzMysbj4GYmZmDap5mPZB5QJiZlZCga8DMTOzBngXlpmZNcQFxMzMGlDzMO2DygXEzKxksrOwfAzEzMwa4B6ImZk1xAWkZIrsEqbGeuyx9Nn7ALZu3ZIcq4jZEbu6trJhw9rkOH+7/67kGJs3P1VInC+cdk5yjF0njuazp5yRnssnP5Ac4/Zly7j/kfT33bidRifHuP43v2HDxieS4zQpfXbE6667jqeeejI5jgrI5Vm+DsTMzBrk4dzNzKwhQ+EgejGThJuZWWG6x8Kq9VYLSVMkrZS0StLMXn5+qKSbJW2VdHQtMV1AzMxKp/biUUsBkdQMXAS8G9gHOFbSPj2a/Q04DvhBrVl6F5aZWQkVfBbWwcCqiLgHQNIVwFHAM2ebRMRf85/VvO/MBcTMrIQKLiCtwP0Vy+3AIalBXUDMzEqozoPo4yQtq1heGBELC07peVxAzMzKJuq+DmRNRBzYz887gEkVy235uiQ+iG5mVjJBdh1Irf9qsBSYLGkvScOBY4BFqXm6gJiZlVBXV2fNt2oiYitwPLAE+CPwo4hYIWmOpCMBJB0kqR34EHCJpBXV4noXlplZ6RQ/nHtELAYW91h3esX9pWS7tmrmAmJmVkIeTNHMzOrWfSV62bmAmJmVkAuImZk1IGAIDKboAmJmVkJDYTh39ddNknQQcH9EPJgv/1/gg8B9wJkR8Vgfj5sKTAVoaWk5YPbs2cmJtrW10d7enhynCNtaLkVMhNPa2kpHR/J1STQ3p3+nmThxAqtXp0+ctOvE1uQY2w9r5ukt1U+zrGa3cWOSYzy1YSM7jNwxOc52Tc3JMdavX8+oUaOS4xShqFze9ra3La9yMV/NtttuWOy00841t1+79uHCtl2PagXkZuCdEfGYpEOBK4AvAq8DXhURVYf8lVRIGV2wYAHTp08vIlSyInKRirkEZ/78C5gx46SkGEXMSHj++XOZOXNWcpzRo3dJjjF79smcffa85DhFzEj4it1Hs/KB9Jn3ipqR8LUHpn/GFDUj4aFvfWtynKJmJDzssMOS40gqtICMGjW25vbr1j0yKAWk2te95opexkfIxlf5CfATSbcObGpmZi9O2TDt5T8GUu1rcLOk7iLzDuDXFT/z8RMzswFS9IRSA6FaEfgh8BtJa4CngN8CSHo5sG6AczMze9Ea8qfxRsS5kq4BJgJXx7O/URPZsRAzMxsAQ76AAETETZLeBvxjfrbOioi4dsAzMzN7MRvqBURSK3Al8DSwPF/9IUnzgPdHRPp5m2Zm1kMQlP8gerUeyL8A34qIyytX5teDXEw2p66ZmRVoqIyFVe0srH16Fg+AiPge8MoBycjMzLaJs7B6LTDKroJLvxzVzMx6tS30QH4u6VJJI7tX5Pe/TY+JSczMrCi19z4Gs9BUKyAnkV3vcZ+k5ZKWA38FngDKMa6Imdk2KKKr5ttgqXYdyBZguqTZwMvz1X+JiI0DnpmZ2YvUNnEQXdJJABHxFPDKiLiju3hImvsC5Gdm9iIUQ6IHUm0X1jEV90/p8bMpBediZma5oVBAqp2FpT7u97ZsZmYFGQq7sKoVkOjjfm/LZmZWkKFQQKpNKNUJbCDrbewAdB88F7B9RFSdiUjSI2QzGKYaB6wpIE4RnMvzlSUPcC59cS69KyqXPSJi1wLiIOmXZHnVak1EvOCHFfotIGUiadlgzLjVG+dS3jzAufTFufSuTLkMNcXMq2pmZi86LiBmZtaQoVRAFg52AhWcy/OVJQ9wLn1xLr0rUy5DypApIBFR94ss6X2SQtIrK9a9TtJ7KjFbXKcAAAhtSURBVJYPk/TGRnORNEbS5yuWd5f043pzbVRvz4ukz+ZD7vdJ0nGS/qWPn82qJwdJxwE/r/cxknavWP6rpHoOGvapkffKQEnJRdKekj46WLlIuqHO9pdLOnogculje6+UdKOkTZIaHlqpTO+XoWbIFJAGHQv8b/5/t9cB76lYPgyoq4D0MAZ4poBExAMRUdMf0UCJiG/nQ+43qq4CAhwH7F6tUQGPecFIqjpb5wtgT6CwAlKviEj5uyhcL6/JY8CXgAWDkI5BfWPOD6UbMAroAPYGVubrhgN/Ax4BbgVOBh7M290KvAXYFfgJsDS/vSl/7JnAZcB1wD3Al/L1VwBP5Y+fT/ZHf2f+s+2B/wfcAdwCvC1ffxzZTI+/BP4MXNBL/gcBV+b3j8q3MTyPeU++/mV5jOXAb8mGm+nOdXpFnNsr8ruzvxyA84HOvP33gZHAL4DbgDuBj/TI82hgPbAyf8wOwDvy3/eO/DkbUcNj/gqcBdycP677dxmZx/hDHvOoXp6ricD1eaw7gbfk64/NY90JzKtov75HLpfn9y8nG2n698CFZOO//U/+u98MvCxvN4PsvXE7cFYv+TTnse7Mt39CldfrcuCbwA1k762j8/U3kQ1meitwQh53fsW2P5O3O4zsfflj4O78dVPF639D/jv8Adiprzi9/B7rq8Xv0f7yitxPz+PfSbaLSPnvf3NF+8ndy8ABwG/y52YJMDFffx3wdWAZMK2PPM8kf7/79gJ/zg52AgP2i8E/AP+a378BOCC/fxzwLxXtnvPmA34AvDm//xLgjxXtbgBGkJ2f/SgwjIqCkbd7ZhmYBlyW338lWfHaPs/hHqAlX74PmNQj/+14tlAsyP8Y3wS8Ffhhvv4aYHJ+/xDg1z1/p/wP+A35/fN5bgHpNQee+wH7QeDSiuWWXp7r64AD8/vbA/cDe+fL3wO+0t9j8uW/Al/M738e+E5+fy7wsfz+GOBPwMgesaYBp+b3m8k+JHfPn+9d8+fy18D7evn9ehaQnwPN+fLvyaZu7v69dgSO4NkPxKa8/aE98jkA+FXF8pgqr9flwH/m8fYBVuXrDwN+XhFnKnBafn8E2YfqXnm7dUBbHuNG4M1kXzjuAQ7KHzM6fy56jdPLa1RZQJ4Xv5f2l/NsAdm5Yv2/Af8nv38t8LqK1/aLZH9HNwC75us/wrN/N9cBF1f5Wz8TF5BBuZWhmz5QjgW+kd+/Il9e3nfzZ7wT2Ed6ZqSW0ZJG5fd/ERGbgE2SHgbGV4n1ZuCfASLibkn3kfWIAK6JiHUAku4C9iD74CVvv1XSXyS9CjiY7BvxoWQfkL/Nc3oj8J8VuY6o3LikMcBOEXFjvuoHwN9XNOk3h9wdwFclzSP7MPttld/5FcC9EfGnfPm7wBfIvkVWc2X+/3LgA/n9I4AjK/Zxb09e2CsetxS4TNIw4KcRcauktwPXRcQj+e/3fbLn76dVcvjPiOiUtBPQGhFXAUTE03mcI/KcbsnbjyL7Jn19RYx7gJdK+mey3tvVNbxeP41sUKO7JPX1vjoCeG3FcYaWfNubgT9ERHue461kX2TWAasjYmn+OzxR8Tv0Fufefp6X3uL/bz/t35YPxrojsDOwAvgZ8B3gHyWdSFYoDiZ7z7wG+FX+3DQDqyti/Uc/27FBtE0WEEk7A28H9pUUZG/IkDSjhoc3Aa/v/sCoiAmwqWJVJ2nPXy2xrgfeDWwh25VyOdnvMiPPc21EvG4gc4iIP0nan+y40TmSromIOQnbrCWfylwEfDAiVvb1oIi4XtKhwHuByyVdSPbh2edDKu5v3+NnG6rkKOC8iLikn3wel/R3wLuAzwIfBr5C/69X5WvR1zhzIuulLXnOSukw6ntv9hqniprjS9oeuJish3m/pDN59nn+CXAGWY9weUQ8mp9MsSIi3tBHyGqviQ2SbfUg+tHAv0XEHhGxZ0RMIvt29RbgSbJdHN16Ll9N1q0GsrO2qmyr5+Mr/ZZsVxqS9ib75tznB2Efj/8KcGP+TXoXsm9rd+bfJu+V9KE8vvIPrWdExFrgSUmH5KsqR1fuz5b82zz5H/fGiPh3sv3m+/fSvvI5WAnsKal7/piPk+3b7u8x/VkCfFF5BZe0X88GkvYAHoqIS8m+4e5Ptr//rZLGSWom64F25/GQpFflUzO/v7eNRsSTQLuk9+XbGCFpxzyfT3b3SiW1StqtRz7jgKaI+AlwGrB/La9XL3o+R0uAz1W8NnurYrbQXqwEJko6KG+/U34gut449eouFmvy5+mZk0ryL2ZLgG+RHR/sznNXSW/I8xkm6dUF5mMDZFstIMcCV/VY95N8/bVku6hulfQRsm71+/Plt5Cd1XGgpNvz3Tqf7W9DEfEo8DtJd0qa3+PHFwNNku4g64Yfl+8Cq9XvyXaTde8euR24IyK6v0H/A/ApSbeR7SI4qpcYnwIuzXc7jKT/b+bdFgK357t99gX+kD/+DOCcXtpfDnw7byPgH8l21dwBdJEdmO7zMZJ26CeXs8n2kd8uaUW+3NNhwG2SbiHbLfKNiFgNzCR7vW8j+7b7X3n7mWTHLm7gubtKevo48CVJt+dtJ0TE1WS7Am/Mf78f8/xC2Apclz8f/86zUyHU8npVuh3olHSbpBPIiuNdwM2S7gQuoZ+eQERszp+Pf863+SuyD/e64tQr/+JyKdnxtyVkuxgrfZ/sfXF1RZ5HA/PyPG+lhjMjJU2Q1A6cCJwmqV3S6KJ+D6tuyIyFZY2RNCoi1uf3Z5Kd3fLlQU7LXsTy41ktETF7sHOxNNvkMRB7jvdKOoXstb6P7Owrs0Eh6Sqy03nfPti5WDr3QMzMrCHb6jEQMzMbYC4gZmbWEBcQMzNriAuImZk1xAXEzMwa8v8BvtuZaix6UCEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfB67N0X-pmM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}